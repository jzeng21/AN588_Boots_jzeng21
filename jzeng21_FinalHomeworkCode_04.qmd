---
title: "jzeng21_OriginalHomeworkCode_04"
format: html
editor: visual
---

## Question 1

```{r}
data <- read.csv("C:\\Users\\jzeng21\\Desktop\\AN588\\AN588_Boots_jzeng21\\KamilarAndCooperData.csv")

data$logHR <- log(data$HomeRange_km2)
data$logBM <- log(data$Body_mass_female_mean)

model <- lm(logHR ~ logBM, data = data)
summary(model)$coefficients
```

Intercept : -9.441231

Slope (logBM) : 1.036432

Equation: log(HomeRange~km^2^~)= -9.441231 + 1.036432 x log(BodyMass~female~)

## Question 2

```{r}
set.seed(123)# reproducitbility
nboot <- 1000 #setting how many boots

#generating sampling distribution for each B coefficient
boot_coefficients <- replicate(nboot, {
  sample_data <- data[sample(nrow(data), replace = TRUE),]
  coef(lm(logHR ~ logBM, data = sample_data))
})


boot_coefficients <- t(boot_coefficients) #transposing the data set 
colnames(boot_coefficients) <- c("Intercept", "logBM")

#compute standard errors and confidence intervals
boot_results <- apply(boot_coefficients, 2, function(x){
  se <- sd(x)
  ci <- quantile(x, probs = c(0.025, 0.0975))
  c(SE = se, LowerCI = ci[1], UpperCI = ci[2])
})

boot_results



```

```{r}
confint(model)
summary(model)$coefficients[, "Std. Error"]
```

Here I made a table to compare the SE from lm and boot we can see that the bootstrap SE is slightly smaller in intercepty and for the logBM as well. This shows that the method used for lm and bootstrap is consistent in results with minor differences.

| Coefficient | SE from `lm()` | SE from Bootstrapping | Comparison |
|-------------|----------------|-----------------------|------------|

|           |        |        |                                  |
|-----------|--------|--------|----------------------------------|
| Intercept | 0.6729 | 0.6030 | Bootstrap SE is slightly smaller |

|       |        |        |                                  |
|-------|--------|--------|----------------------------------|
| logBM | 0.0849 | 0.0785 | Bootstrap SE is slightly smaller |

Here I made a table for the confidence intervals as well, here the table shows that the bootstrap confidence intervals are narrower which = less variability.

| Coefficient | 95% CI from `lm()` | 95% CI from Bootstrapping | Comparison |
|------------------|------------------|------------------|------------------|
| Intercept | \[-10.6796, -8.2029\] *(calc'd)* | \[-10.6796, -10.2130\] | Bootstrap CI is narrower & lower |
| logBM | \[0.8895, 1.1833\] *(calc'd)* | \[0.8895, 0.9332\] | Bootstrap CI is much narrower |

Overall results show robustness.

## Extra Credit 1

```{r}
boot_lm <- function(d, m, conf.level = 0.95, n = 1000) {
  # d: dataset
  # m: model formula as a string (e.g., "logHR ~ logBM")
  # conf.level: confidence level for intervals (default = 95%)
  # n: number of bootstrap samples (default = 1000)

  # Fit the linear model to the original data
  mod <- lm(as.formula(m), data = d)

  # Extract original model coefficients
  full_coefs <- coef(mod)

  # Extract standard errors of original model coefficients
  full_se <- summary(mod)$coefficients[, "Std. Error"]

  # Compute confidence intervals for the original model
  full_ci <- confint(mod, level = conf.level)
  
  # Run bootstrap: repeat n times
  boot_out <- replicate(n, {
    # Sample rows of the data with replacement
    samp <- d[sample(nrow(d), replace = TRUE), ]
    # Fit the model to the bootstrap sample and extract coefficients
    coef(lm(as.formula(m), data = samp))
  })

  # Transpose to make rows = samples, columns = coefficients
  boot_out <- t(boot_out)

  # Compute standard deviation of each coefficient across bootstrap samples (bootstrap SE)
  se_boot <- apply(boot_out, 2, sd)

  # Compute bootstrap confidence intervals using quantiles
  ci_boot <- apply(boot_out, 2, function(x) quantile(x, probs = c((1 - conf.level) / 2, 1 - (1 - conf.level) / 2)))

  # Compute mean of bootstrap coefficients
  mean_boot <- apply(boot_out, 2, mean)
  
  # Create a summary dataframe combining original and bootstrap estimates
  df <- data.frame(
    Coef = names(full_coefs),                # Coefficient names
    Original_Estimate = full_coefs,          # Original model coefficients
    Original_SE = full_se,                   # Original standard errors
    Original_LowerCI = full_ci[, 1],         # Original lower CI
    Original_UpperCI = full_ci[, 2],         # Original upper CI
    Boot_Mean = mean_boot,                   # Mean of bootstrap estimates
    Boot_SE = se_boot,                       # Bootstrap standard errors
    Boot_LowerCI = ci_boot[1, ],             # Bootstrap lower CI (e.g., 2.5%)
    Boot_UpperCI = ci_boot[2, ]              # Bootstrap upper CI (e.g., 97.5%)
  )

  # Return the summary dataframe
  return(df)
}

# Example usage:
boot_lm(data, "logHR ~ logBM")


```

## Extra Credit 2

```{r}
library(ggplot2)
library(reshape2)  # Note: reshape2 isn't actually needed here, but it doesn't hurt to load.

# Function to visualize how bootstrap estimates converge as sample size increases
boot_convergence <- function(d, m) {
  # Fit the original linear model to get the baseline coefficients
  mod <- lm(as.formula(m), data = d)
  betas <- coef(mod)

  # Define a sequence of bootstrap sample sizes (e.g., 10 to 200 in steps of 10)
  boot_seq <- seq(10, 200, by = 10)
  
  # Create a list to store results from each bootstrap size
  out_list <- list()

  # Loop over each value in boot_seq
  for (n in boot_seq) {
    # For each n, perform 'n' bootstrap replicates
    boots <- replicate(n, {
      # Sample data with replacement
      samp <- d[sample(nrow(d), replace = TRUE), ]
      # Fit model to the bootstrap sample and return coefficients
      coef(lm(as.formula(m), data = samp))
    })

    # Transpose matrix so rows = bootstrap replicates, columns = coefficients
    boots <- t(boots)

    # Calculate mean of each coefficient across bootstrap samples
    mean_boot <- apply(boots, 2, mean)

    # Compute 95% confidence intervals from bootstrap samples
    ci_boot <- apply(boots, 2, function(x) quantile(x, probs = c(0.025, 0.975)))

    # Create a dataframe to hold results for this bootstrap size
    temp <- data.frame(
      NumBoots = n,                        # Number of bootstrap samples
      Coef = names(mean_boot),            # Coefficient names
      Mean = mean_boot,                   # Mean coefficient estimate
      Lower = ci_boot[1, ],               # Lower 2.5% quantile
      Upper = ci_boot[2, ],               # Upper 97.5% quantile
      Original = betas                    # Original coefficient estimates (repeated)
    )

    # Append to the output list
    out_list[[length(out_list) + 1]] <- temp
  }

  # Combine all results into a single dataframe
  final_df <- do.call(rbind, out_list)
  return(final_df)
}

# Run the function using your data and formula
plot_df <- boot_convergence(data, "logHR ~ logBM")

# Create the convergence plot
ggplot(plot_df, aes(x = NumBoots, y = Mean, color = Coef)) +
  # Plot the line of mean bootstrap estimate for each coefficient
  geom_line() +
  # Add shaded ribbons for 95% bootstrap confidence intervals
  geom_ribbon(aes(ymin = Lower, ymax = Upper, fill = Coef), alpha = 0.2, linetype = 0) +
  # Add dashed horizontal lines for the original coefficient values
  geom_hline(aes(yintercept = Original, color = Coef), linetype = "dashed") +
  # Separate facets for each coefficient, y-axis scales independent
  facet_wrap(~ Coef, scales = "free_y") +
  # Add labels and clean theme
  labs(title = "Bootstrap CI Convergence", 
       y = "Coefficient Value", 
       x = "Number of Bootstraps") +
  theme_minimal()

```
